---
description: Use Flows to implement streaming, asynchronous data transformation pipelines. Flows integrate with I/O, provide a various ways to integrate with data sources and sinks, as well as declarative intermediate transformations. 
globs: 
alwaysApply: false
---
# Ox Streaming Using Flows

Use Flows to implement streaming, asynchronous data transformation pipelines. Flows integrate with I/O, provide various ways to integrate with data sources and sinks, as well as declarative intermediate transformations.

## Flow Fundamentals

Flows are lazy, composable data transformation pipelines:

```scala
import ox.flow.Flow
import scala.concurrent.duration.*

// Create flows from various sources
val numbersFlow = Flow.fromValues(1, 2, 3, 4, 5)
val infiniteFlow = Flow.iterate(0)(_ + 1)
val timedFlow = Flow.tick(1.second, "ping")

// Flows are lazy - nothing happens until run
val pipeline = numbersFlow
  .filter(_ % 2 == 0)
  .map(_ * 2)
  .take(3)

// Run the flow to execute the pipeline
val results = pipeline.runToList() // List(4, 8)
```

## Creating Flows

Multiple ways to create flows for different data sources:

```scala
// From collections
val collectionFlow = Flow.fromValues(1, 2, 3, 4)

// From iterables
val iterableFlow = Flow.fromIterable(List("a", "b", "c"))

// Infinite sequences
val naturalNumbers = Flow.iterate(0)(_ + 1)
val timed = Flow.tick(500.millis, "heartbeat")

// From channels
val channelFlow = supervised {
  val channel = Channel.buffered
  fork {
    channel.send("hello")
    channel.send("world")
    channel.done()
  }
  Flow.fromSource(channel)
}

// Custom emission logic
val customFlow = Flow.usingEmit { emit =>
  emit("start")
  for (i <- 1 to 5) {
    emit(s"item-$i")
  }
  emit("end")
}
```

## Flow Transformations

Flows support rich transformation operations:

```scala
// Basic transformations
val pipeline = Flow.fromValues(1, 2, 3, 4, 5, 6)
  .filter(_ % 2 == 0)           // Keep even numbers
  .map(_ * 2)                   // Double them
  .take(3)                      // Take first 3
  .drop(1)                      // Skip first 1

// Stateful transformations
val runningSum = Flow.fromValues(1, 2, 3, 4, 5)
  .mapStateful(0) { (sum, value) =>
    val newSum = sum + value
    (newSum, newSum) // (newState, emittedValue)
  }

// Grouping and windowing
val grouped = Flow.iterate(0)(_ + 1)
  .take(10)
  .grouped(3) // Group into batches of 3

// Text processing
val textFlow = Flow.fromValues("hello", "world", "!")
  .intersperse(" ")             // Add spaces between elements
  .map(_.toUpperCase)
```

## I/O Integration

Flows excel at I/O operations and file processing:

```scala
// File processing
val fileFlow = Flow
  .fromInputStream(new FileInputStream("input.txt"))
  .linesUtf8                    // Split into lines
  .filter(_.nonEmpty)           // Skip empty lines
  .map(_.trim)                  // Trim whitespace
  .mapPar(4)(processLine)       // Process lines in parallel

// HTTP requests
def processUrls(urls: List[String]): Unit = {
  Flow.fromIterable(urls)
    .mapPar(8) { url =>
      httpClient.get(url)       // Concurrent HTTP requests
    }
    .filter(_.status == 200)    // Only successful responses
    .map(_.body)                // Extract response bodies
    .runForeach(println)        // Print each body
}
```

## Concurrency in Flows

Flows provide built-in concurrency control:

```scala
// Parallel processing
val processedFlow = Flow.fromValues(items)
  .mapPar(4) { item =>
    expensiveProcessing(item)   // Up to 4 concurrent operations
  }

// Buffering for performance
val bufferedFlow = Flow.fromValues(items)
  .buffer(100)                  // Buffer up to 100 elements
  .map(processItem)

// Merging multiple flows
val flow1 = Flow.tick(100.millis, "fast")
val flow2 = Flow.tick(300.millis, "slow")
val merged = flow1.merge(flow2)   // Merge both flows
```

## Running Flows

Different ways to execute flows based on your needs:

```scala
// Collect all results
val results: List[Int] = flow.runToList()

// Process with side effects
flow.runForeach(println)

// Drain (consume without collecting)
flow.runDrain()

// Run to channel for integration
supervised {
  val channel = flow.runToChannel()
  // Use channel in other parts of your application
}

// Custom collection
val sum = flow.runFold(0)(_ + _)
```

## Error Handling in Flows

Flows integrate with Ox's error handling patterns:

```scala
// Exception handling
val safeFlow = Flow.fromValues(items)
  .map { item =>
    try {
      processItem(item)
    } catch {
      case e: ProcessingException => 
        logger.warn(s"Failed to process $item: ${e.getMessage}")
        None
    }
  }
  .collect { case Some(result) => result }

// Using Either for error propagation
val eitherFlow = Flow.fromValues(items)
  .map { item =>
    Either.catchNonFatal(processItem(item))
  }
  .collect { case Right(result) => result }
```

## Real-World Examples

### Log Processing Pipeline

```scala
class LogProcessor {
  def processLogFile(filePath: String): Unit = {
    Flow.fromInputStream(new FileInputStream(filePath))
      .linesUtf8
      .filter(_.nonEmpty)
      .map(parseLogLine)
      .collect { case Some(logEntry) => logEntry }
      .filter(_.level >= LogLevel.WARN)
      .mapPar(4) { entry =>
        // Parallel processing of log entries
        enrichLogEntry(entry)
      }
      .grouped(100)
      .runForeach { batch =>
        database.insertLogEntries(batch)
      }
  }
}
```

### Data ETL Pipeline

```scala
class DataETLPipeline {
  def processData(): Unit = supervised {
    // Extract
    val sourceFlow = Flow.fromIterable(dataSourceUrls)
      .mapPar(3) { url => 
        httpClient.get(url).body 
      }
    
    // Transform
    val transformedFlow = sourceFlow
      .map(parseJson)
      .collect { case Some(data) => data }
      .filter(validateData)
      .map(transformData)
      .buffer(1000)
    
    // Load
    transformedFlow
      .grouped(100)
      .runForeach { batch =>
        database.insertBatch(batch)
      }
  }
}
```

### Message Queue Processing

```scala
class MessageProcessor {
  def processMessages(): Unit = supervised {
    Flow.usingEmit[Message] { emit =>
      while (true) {
        val message = messageQueue.poll()
        if (message != null) {
          emit(message)
        } else {
          Thread.sleep(100) // Brief pause if no messages
        }
      }
    }
    .mapPar(8) { message =>
      processMessage(message)
    }
    .runDrain()
  }
}
```

## Flow Composition Patterns

### Producer-Consumer with Flows

```scala
// Producer flow
val producer = Flow.usingEmit[WorkItem] { emit =>
  while (hasWork()) {
    emit(generateWorkItem())
  }
}

// Consumer processing
producer
  .mapPar(4) { workItem =>
    processWorkItem(workItem)
  }
  .runForeach(storeResult)
```

### Fan-out Pattern

```scala
// Split flow into multiple processing streams
val sourceFlow = Flow.fromValues(items)

supervised {
  val channel1 = sourceFlow.filter(_.priority == High).runToChannel()
  val channel2 = sourceFlow.filter(_.priority == Normal).runToChannel()
  
  fork { processHighPriority(Flow.fromSource(channel1)) }
  fork { processNormal(Flow.fromSource(channel2)) }
}
```

## Best Practices

1. **Use flows for data pipelines** - they're designed for streaming transformations
2. **Leverage lazy evaluation** - flows only execute when run
3. **Control concurrency** with `mapPar`, `buffer`, and `merge`
4. **Handle errors appropriately** - use `collect` or `Either` patterns
5. **Use appropriate run methods** - `runToList` for small datasets, `runForeach` for processing
6. **Buffer strategically** - use `buffer()` for performance with varying processing speeds
7. **Integrate with channels** - use `runToChannel()` for complex integrations
8. **Test with small datasets** - verify flow logic before scaling up
